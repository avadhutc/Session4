{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eva_Session_4_Fourth_Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avadhutc/Session4/blob/master/code/Eva_Session_4_Fourth_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oHciwvYI5tO",
        "colab_type": "text"
      },
      "source": [
        "## This Pytorch notebook implementation similar to Keras fourth notebook\n",
        "### This notebook uses:\n",
        "#### 1. batch size 128\n",
        "#### 2. Epochs =50\n",
        "#### 3. Reduce Lr on Plateau\n",
        "#### 4. BatchNormalization\n",
        "#### 5. Dropout(0.2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkjPeSO0I3hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import torch and torchvision package for use\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNlKnEQLxwc",
        "colab_type": "code",
        "outputId": "9096e88c-2004-4e06-e99e-5af0fca6381b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqsFFrMvlCxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "batch_size = 128\n",
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "learning_rate = 0.01\n",
        "## global mean and std (0.1307,), (0.3081,)\n",
        "# normalize training and test dataset between 0 and 1\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train= True,\n",
        "                                      download= True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train= False,\n",
        "                                      download= True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "image_datasets = {\n",
        "    'train': trainset,    \n",
        "    'validation': testset    \n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size = batch_size, shuffle=True),    \n",
        "    'validation': torch.utils.data.DataLoader(image_datasets['validation'], batch_size = batch_size, shuffle=False)\n",
        "}\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrVRehIbtZF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples = enumerate(testloader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89ReeIbPtd9U",
        "colab_type": "code",
        "outputId": "9f16d691-962c-49d5-ebcb-ded88b262d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_data.shape"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwheO8qstjjD",
        "colab_type": "code",
        "outputId": "e6a8fa62-0134-4194-ed14-abe25ef0bf5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  fig"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAELCAYAAAAP/iu7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAE9/8Bdpu4lNJZh5UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kNxLQxKtkHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya4yhPZbtkKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17JqVt0dlC0B",
        "colab_type": "code",
        "outputId": "fb4e2465-08e8-4f88-a468-4f6100e0eb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "import matplotlib.pyplot as plt# import pyplot alias plt for plotting\n",
        "import numpy as np # import numpy package alias np\n",
        "\n",
        "# functions ti show an image\n",
        "\n",
        "def imshow(img): \n",
        "  img = img/2 + 0.5 # unnormalize\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show() # display image\n",
        "  \n",
        "  \n",
        "#get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images,labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(32)))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJgAAAD8CAYAAACLp21tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEFT+P/OwYHJluRhVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "    4     7     3     6     0     3     9     9     4     1     9     6     7     1     8     1     6     1     5     1     0     1     0     1     4     2     9     4     3     0     7     8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkH8Xc6AlC2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySOkBQiuFLav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50WZ81H9FOLt",
        "colab_type": "code",
        "outputId": "b8c68bf2-4c58-4fde-9790-c56eab4ced8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "          super(Net, self).__init__()        \n",
        "          self.conv1 = nn.Sequential(nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=0), nn.ReLU()) # Input Channel dimension= 1, output channel dimension= 32, RF = 3        \n",
        "          self.bnrm1 = nn.Sequential(nn.BatchNorm2d(16))\n",
        "          self.drop1 = nn.Sequential(nn.Dropout2d(0.2))\n",
        "          self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0), nn.ReLU())# Input Channel dimension= 16, output channel dimension= 32, RF = 5\n",
        "          self.bnrm2 = nn.Sequential(nn.BatchNorm2d(32))\n",
        "          self.drop2 = nn.Sequential(nn.Dropout2d(0.2))\n",
        "          self.pool1 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))# Input Channel dimension= 32, output channel dimension= 32, RF = 6\n",
        "          self.conv3 = nn.Sequential(nn.Conv2d(32, 10, kernel_size=1, stride=1, padding=0), nn.ReLU())  # Input Channel dimension= 32, output channel dimension= 10, RF = 6       \n",
        "          self.conv4 = nn.Sequential(nn.Conv2d(10, 64, kernel_size=3, stride=1, padding=0), nn.ReLU())# Input Channel dimension= 10, output channel dimension= 64, RF = 10\n",
        "          self.bnrm3 = nn.Sequential(nn.BatchNorm2d(64))\n",
        "          self.drop3 = nn.Sequential(nn.Dropout2d(0.2))\n",
        "          self.pool2 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))# Input Channel dimension= 64, output channel dimension= 64, RF = 11\n",
        "          self.conv5 = nn.Sequential(nn.Conv2d(64, 10, kernel_size=1, stride=1, padding=0)) # Input Channel dimension= 64, output channel dimension= 10, RF = 11   \n",
        "          self.conv6 = nn.Sequential(nn.Conv2d(10, 10, kernel_size=5, stride=1, padding=0)) # Input Channel dimension= 10, output channel dimension= 10, RF = 11 + 8 + 8 = 27               \n",
        "          \n",
        "          \n",
        "        def forward(self, x):\n",
        "          x = self.conv1(x)\n",
        "          x = self.bnrm1(x)\n",
        "          x = self.drop1(x)\n",
        "          x = self.conv2(x)\n",
        "          x = self.bnrm2(x)\n",
        "          x = self.drop2(x)\n",
        "          x = self.pool1(x)\n",
        "          x = self.conv3(x)\n",
        "          x = self.conv4(x)\n",
        "          x = self.bnrm3(x)\n",
        "          x = self.drop3(x)\n",
        "          x = self.pool2(x)\n",
        "          x = self.conv5(x)\n",
        "          x = self.conv6(x)\n",
        "          \n",
        "          x = x.reshape(x.size(0), -1) #flatten image dimension into 1\n",
        "          \n",
        "          #print(x.size(0))\n",
        "          #x = x.view(x.size(0), -1)# or x = x.view(32, -1) as batch_size=32 or x.view(x.size(0), -1)\n",
        "          \n",
        "          x = F.log_softmax(x)# softmax to assign confidence score of each class\n",
        "          return x\n",
        "        \n",
        "model = Net().to(device)\n",
        "summary(model, (1, 28, 28))    \n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             160\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "         Dropout2d-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 32, 24, 24]           4,640\n",
            "              ReLU-6           [-1, 32, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 32, 24, 24]              64\n",
            "         Dropout2d-8           [-1, 32, 24, 24]               0\n",
            "         MaxPool2d-9           [-1, 32, 12, 12]               0\n",
            "           Conv2d-10           [-1, 10, 12, 12]             330\n",
            "             ReLU-11           [-1, 10, 12, 12]               0\n",
            "           Conv2d-12           [-1, 64, 10, 10]           5,824\n",
            "             ReLU-13           [-1, 64, 10, 10]               0\n",
            "      BatchNorm2d-14           [-1, 64, 10, 10]             128\n",
            "        Dropout2d-15           [-1, 64, 10, 10]               0\n",
            "        MaxPool2d-16             [-1, 64, 5, 5]               0\n",
            "           Conv2d-17             [-1, 10, 5, 5]             650\n",
            "           Conv2d-18             [-1, 10, 1, 1]           2,510\n",
            "================================================================\n",
            "Total params: 14,338\n",
            "Trainable params: 14,338\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.16\n",
            "Params size (MB): 0.05\n",
            "Estimated Total Size (MB): 1.22\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWumAS_HlC94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_interval = 200\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import time\n",
        "import copy\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   ## cross entropy loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003,betas=(0.9, 0.999), eps=1e-8,weight_decay=0, amsgrad=False )##adam optimizer, learning rate = 1e-3\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,mode='max', patience=7, factor=0.1,  verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZpAyIkGlDAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##loop over 50 epochs, one batch has 32 samples, avg loss and accuracy printed over 3200 samples\n",
        "log_interval = 200\n",
        "def train_model(model, criterion, optimizer, num_epochs=3):\n",
        "      for epoch in range(0, num_epochs):  #loop over the dataset multiple times\n",
        "              print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "              print('-' * 10)\n",
        "        \n",
        "              for phase in ['train', 'validation']:\n",
        "                  if phase == 'train':\n",
        "                      model.train()\n",
        "                  else:\n",
        "                      model.eval()\n",
        "\n",
        "                  running_loss = 0.0\n",
        "                  running_corrects = 0\n",
        "                  correct = 0\n",
        "                  total = 0\n",
        "                  \n",
        "                  for batch_idx, data in enumerate(dataloaders[phase], 0):\n",
        "                      #get the inputs\n",
        "                      inputs, labels = data\n",
        "\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      \n",
        "                      # Clear gradients w.r.t. parameters\n",
        "                      #optimizer.zero_grad()\n",
        "                      \n",
        "                      #forward + backward _ optimize\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      \n",
        "                      if phase == 'train':\n",
        "                          #zero the parameter gradients\n",
        "                          optimizer.zero_grad()\n",
        "                          # Backpropagation\n",
        "                          loss.backward()\n",
        "                          optimizer.step()\n",
        "\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      running_loss += loss.detach() * inputs.size(0)\n",
        "                      running_corrects += torch.sum(preds == labels.data)\n",
        "                  \n",
        "                  epoch_loss = running_loss / len(image_datasets[phase])\n",
        "                  epoch_acc = running_corrects.float() / len(image_datasets[phase])\n",
        "\n",
        "                  print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
        "                                                        epoch_loss.item(),\n",
        "       \n",
        "                                                              epoch_acc.item()))\n",
        "                  print('-'*20)\n",
        "                  scheduler.step(epoch_acc)\n",
        "      return model          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nyj0o-XlDC8",
        "colab_type": "code",
        "outputId": "efca5265-c842-44a3-efbb-0691d800b226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5619
        }
      },
      "source": [
        "model_trained = train_model(model, criterion, optimizer, num_epochs=50)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 0.1783, acc: 0.9480\n",
            "--------------------\n",
            "validation loss: 0.0548, acc: 0.9814\n",
            "--------------------\n",
            "Epoch 2/50\n",
            "----------\n",
            "train loss: 0.0711, acc: 0.9787\n",
            "--------------------\n",
            "validation loss: 0.0370, acc: 0.9872\n",
            "--------------------\n",
            "Epoch 3/50\n",
            "----------\n",
            "train loss: 0.0590, acc: 0.9823\n",
            "--------------------\n",
            "validation loss: 0.0359, acc: 0.9886\n",
            "--------------------\n",
            "Epoch 4/50\n",
            "----------\n",
            "train loss: 0.0525, acc: 0.9835\n",
            "--------------------\n",
            "validation loss: 0.0393, acc: 0.9878\n",
            "--------------------\n",
            "Epoch 5/50\n",
            "----------\n",
            "train loss: 0.0460, acc: 0.9861\n",
            "--------------------\n",
            "validation loss: 0.0332, acc: 0.9893\n",
            "--------------------\n",
            "Epoch 6/50\n",
            "----------\n",
            "train loss: 0.0420, acc: 0.9873\n",
            "--------------------\n",
            "validation loss: 0.0296, acc: 0.9894\n",
            "--------------------\n",
            "Epoch 7/50\n",
            "----------\n",
            "train loss: 0.0384, acc: 0.9879\n",
            "--------------------\n",
            "validation loss: 0.0313, acc: 0.9909\n",
            "--------------------\n",
            "Epoch 8/50\n",
            "----------\n",
            "train loss: 0.0373, acc: 0.9887\n",
            "--------------------\n",
            "validation loss: 0.0279, acc: 0.9905\n",
            "--------------------\n",
            "Epoch 9/50\n",
            "----------\n",
            "train loss: 0.0362, acc: 0.9887\n",
            "--------------------\n",
            "validation loss: 0.0283, acc: 0.9911\n",
            "--------------------\n",
            "Epoch 10/50\n",
            "----------\n",
            "train loss: 0.0331, acc: 0.9899\n",
            "--------------------\n",
            "validation loss: 0.0319, acc: 0.9892\n",
            "--------------------\n",
            "Epoch 11/50\n",
            "----------\n",
            "train loss: 0.0320, acc: 0.9900\n",
            "--------------------\n",
            "validation loss: 0.0261, acc: 0.9912\n",
            "--------------------\n",
            "Epoch 12/50\n",
            "----------\n",
            "train loss: 0.0301, acc: 0.9905\n",
            "--------------------\n",
            "validation loss: 0.0268, acc: 0.9914\n",
            "--------------------\n",
            "Epoch 13/50\n",
            "----------\n",
            "train loss: 0.0293, acc: 0.9909\n",
            "--------------------\n",
            "validation loss: 0.0270, acc: 0.9912\n",
            "--------------------\n",
            "Epoch 14/50\n",
            "----------\n",
            "train loss: 0.0282, acc: 0.9912\n",
            "--------------------\n",
            "validation loss: 0.0279, acc: 0.9913\n",
            "--------------------\n",
            "Epoch 15/50\n",
            "----------\n",
            "train loss: 0.0282, acc: 0.9914\n",
            "--------------------\n",
            "validation loss: 0.0272, acc: 0.9922\n",
            "--------------------\n",
            "Epoch 16/50\n",
            "----------\n",
            "train loss: 0.0285, acc: 0.9906\n",
            "--------------------\n",
            "validation loss: 0.0272, acc: 0.9922\n",
            "--------------------\n",
            "Epoch 17/50\n",
            "----------\n",
            "train loss: 0.0263, acc: 0.9916\n",
            "--------------------\n",
            "validation loss: 0.0308, acc: 0.9902\n",
            "--------------------\n",
            "Epoch 18/50\n",
            "----------\n",
            "train loss: 0.0267, acc: 0.9913\n",
            "--------------------\n",
            "validation loss: 0.0267, acc: 0.9916\n",
            "--------------------\n",
            "Epoch 19/50\n",
            "----------\n",
            "train loss: 0.0254, acc: 0.9917\n",
            "--------------------\n",
            "validation loss: 0.0293, acc: 0.9915\n",
            "--------------------\n",
            "Epoch    37: reducing learning rate of group 0 to 3.0000e-04.\n",
            "Epoch 20/50\n",
            "----------\n",
            "train loss: 0.0166, acc: 0.9947\n",
            "--------------------\n",
            "validation loss: 0.0225, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 21/50\n",
            "----------\n",
            "train loss: 0.0144, acc: 0.9952\n",
            "--------------------\n",
            "validation loss: 0.0225, acc: 0.9936\n",
            "--------------------\n",
            "Epoch 22/50\n",
            "----------\n",
            "train loss: 0.0146, acc: 0.9950\n",
            "--------------------\n",
            "validation loss: 0.0222, acc: 0.9933\n",
            "--------------------\n",
            "Epoch 23/50\n",
            "----------\n",
            "train loss: 0.0128, acc: 0.9957\n",
            "--------------------\n",
            "validation loss: 0.0216, acc: 0.9938\n",
            "--------------------\n",
            "Epoch 24/50\n",
            "----------\n",
            "train loss: 0.0119, acc: 0.9964\n",
            "--------------------\n",
            "validation loss: 0.0212, acc: 0.9940\n",
            "--------------------\n",
            "Epoch 25/50\n",
            "----------\n",
            "train loss: 0.0112, acc: 0.9964\n",
            "--------------------\n",
            "validation loss: 0.0216, acc: 0.9936\n",
            "--------------------\n",
            "Epoch 26/50\n",
            "----------\n",
            "train loss: 0.0119, acc: 0.9961\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9935\n",
            "--------------------\n",
            "Epoch 27/50\n",
            "----------\n",
            "train loss: 0.0114, acc: 0.9964\n",
            "--------------------\n",
            "validation loss: 0.0218, acc: 0.9934\n",
            "--------------------\n",
            "Epoch 28/50\n",
            "----------\n",
            "train loss: 0.0116, acc: 0.9963\n",
            "--------------------\n",
            "Epoch    54: reducing learning rate of group 0 to 3.0000e-05.\n",
            "validation loss: 0.0224, acc: 0.9932\n",
            "--------------------\n",
            "Epoch 29/50\n",
            "----------\n",
            "train loss: 0.0104, acc: 0.9967\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9929\n",
            "--------------------\n",
            "Epoch 30/50\n",
            "----------\n",
            "train loss: 0.0101, acc: 0.9968\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9932\n",
            "--------------------\n",
            "Epoch 31/50\n",
            "----------\n",
            "train loss: 0.0103, acc: 0.9965\n",
            "--------------------\n",
            "validation loss: 0.0219, acc: 0.9932\n",
            "--------------------\n",
            "Epoch 32/50\n",
            "----------\n",
            "train loss: 0.0099, acc: 0.9968\n",
            "--------------------\n",
            "validation loss: 0.0220, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 33/50\n",
            "----------\n",
            "train loss: 0.0098, acc: 0.9968\n",
            "--------------------\n",
            "validation loss: 0.0220, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 34/50\n",
            "----------\n",
            "train loss: 0.0098, acc: 0.9967\n",
            "--------------------\n",
            "Epoch    66: reducing learning rate of group 0 to 3.0000e-06.\n",
            "validation loss: 0.0220, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 35/50\n",
            "----------\n",
            "train loss: 0.0097, acc: 0.9972\n",
            "--------------------\n",
            "validation loss: 0.0220, acc: 0.9933\n",
            "--------------------\n",
            "Epoch 36/50\n",
            "----------\n",
            "train loss: 0.0096, acc: 0.9970\n",
            "--------------------\n",
            "validation loss: 0.0220, acc: 0.9932\n",
            "--------------------\n",
            "Epoch 37/50\n",
            "----------\n",
            "train loss: 0.0098, acc: 0.9967\n",
            "--------------------\n",
            "validation loss: 0.0220, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 38/50\n",
            "----------\n",
            "train loss: 0.0092, acc: 0.9967\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9934\n",
            "--------------------\n",
            "Epoch 39/50\n",
            "----------\n",
            "train loss: 0.0095, acc: 0.9970\n",
            "--------------------\n",
            "Epoch    76: reducing learning rate of group 0 to 3.0000e-07.\n",
            "validation loss: 0.0221, acc: 0.9933\n",
            "--------------------\n",
            "Epoch 40/50\n",
            "----------\n",
            "train loss: 0.0101, acc: 0.9966\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9936\n",
            "--------------------\n",
            "Epoch 41/50\n",
            "----------\n",
            "train loss: 0.0097, acc: 0.9968\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9935\n",
            "--------------------\n",
            "Epoch 42/50\n",
            "----------\n",
            "train loss: 0.0102, acc: 0.9966\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9932\n",
            "--------------------\n",
            "Epoch 43/50\n",
            "----------\n",
            "train loss: 0.0097, acc: 0.9968\n",
            "--------------------\n",
            "Epoch    84: reducing learning rate of group 0 to 3.0000e-08.\n",
            "validation loss: 0.0221, acc: 0.9933\n",
            "--------------------\n",
            "Epoch 44/50\n",
            "----------\n",
            "train loss: 0.0092, acc: 0.9969\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9932\n",
            "--------------------\n",
            "Epoch 45/50\n",
            "----------\n",
            "train loss: 0.0099, acc: 0.9969\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9934\n",
            "--------------------\n",
            "Epoch 46/50\n",
            "----------\n",
            "train loss: 0.0096, acc: 0.9971\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 47/50\n",
            "----------\n",
            "train loss: 0.0096, acc: 0.9970\n",
            "--------------------\n",
            "Epoch    92: reducing learning rate of group 0 to 3.0000e-09.\n",
            "validation loss: 0.0221, acc: 0.9933\n",
            "--------------------\n",
            "Epoch 48/50\n",
            "----------\n",
            "train loss: 0.0102, acc: 0.9966\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9930\n",
            "--------------------\n",
            "Epoch 49/50\n",
            "----------\n",
            "train loss: 0.0097, acc: 0.9970\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9931\n",
            "--------------------\n",
            "Epoch 50/50\n",
            "----------\n",
            "train loss: 0.0093, acc: 0.9971\n",
            "--------------------\n",
            "validation loss: 0.0221, acc: 0.9935\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v1R9HdzlDFm",
        "colab_type": "code",
        "outputId": "76b353ec-079e-48fc-b170-ef7b526dc7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#model score accuarcy on test data\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    \n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    \n",
        "print('Accuracy of the network on the 10000 test images: %.6f %%' % (100 * correct / total))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 99.350000 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDaRrukIVvq",
        "colab_type": "text"
      },
      "source": [
        "## Maximum Validation Accuracy achieved 99.40% @ 24th epoch with 14.3K parameters"
      ]
    }
  ]
}